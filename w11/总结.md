1.embedding理解
  我对word embedding的简单理解是单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中，
一个萝卜一个坑。之所以希望把每个单词变成一个向量，目的还是为了方便计算，比如“求单词A的同义词”，就可以通过“求与单词A在cos距离下
最相似的向量”来做到。

2.embedding结果
![输入图片说明](https://images.gitee.com/uploads/images/2018/0712/175901_1a963465_1773351.png "屏幕截图.png")
  图中圈起来的为相关性的字

3.代码
  主要的代码补全是在model.py。在作业中需要我们补充的几部分代码为：
  ①tf.variable_scope('rnn'):
    `stacked_rnn = []
            for iLayer in range(self.rnn_layers):
                lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(128, forget_bias=1.0, state_is_tuple=True)
                lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=self.keep_prob)
                stacked_rnn.append(lstm_cell)

            cell = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn, state_is_tuple=True)

            self.init_state = cell.zero_state(self.batch_size, dtype=tf.float32)

            rnn_inputs = tf.one_hot(self.X, self.num_words)
            state = self.init_state
            outputs_tensor, state = tf.nn.dynamic_rnn(cell, inputs=rnn_inputs, initial_state=state, time_major=False)
            outputs_state_tensor = outputs_tensor[:, -1, :]
            self.outputs_state_tensor = outputs_state_tensor
            self.final_state = state
        seq_output = tf.concat(outputs_tensor, 1)
        seq_output_final = tf.reshape(seq_output, [-1, self.dim_embedding])`
    主要思路是：构造多层LSTM，通过zero_state对状态进行初始化，创建递归神经网络。最后用tf.reshape将串接的outputs转为一维向量。
  ②tf.variable_scope('softmax'):
    `W = tf.get_variable('W', [128, self.num_words], 
                                initializer=tf.random_normal_initializer(stddev=0.01))
            bias = tf.get_variable('b', [self.num_words], initializer=tf.constant_initializer(0.0))
            logits = tf.matmul(seq_output_final, W) + bias`
    这部分就是定义权重和偏置，并计算得出最后的logits。
4.训练结果
    详见部分log输出.txt